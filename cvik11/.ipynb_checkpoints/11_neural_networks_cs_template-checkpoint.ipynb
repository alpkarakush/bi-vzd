{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronové sítě"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many frameworks for neural networks: `tensorflow` , `pytorch`, `mxnet`. Also `scikit-learn` has its own neural network implementation. But it's very useful to know the fundamentals, because it allows us to use better those tools, and to extend them if needed.\n",
    "\n",
    "In this tutorial we will implement a toy version of a neural network in pure `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definujme aktivační funkci a počáteční dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoida společně s její derivací\n",
    "def sigmoid(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "xxx = np.linspace(-10,10,100)\n",
    "plt.plot(xxx, sigmoid(xxx))\n",
    "plt.plot(xxx, sigmoid(xxx, deriv = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,1,0,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_X_y(X,y):\n",
    "    color = {0:'blue', 1:'red'}\n",
    "    colors = [color[y[ix][0]] for ix in range(len(y))]\n",
    "    plt.scatter(X[:,0],X[:,1], c=colors)\n",
    "    plt.title(\"Data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_X_y(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pojďme definovat naši třídu pro neuronovou síť: jediným parametrem bude počet vstupů\n",
    "\n",
    "Implementujeme čtyři její metody:\n",
    "- `forward`: multiplying the input by the weights across layers.\n",
    "- `backward`: performing update of the weights, given the output.\n",
    "- `fit`: which combines these two in a loop.\n",
    "- `predict`: an alias for `forward` basically, just to make it `scikit-learn`-like :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, input_dim):\n",
    "        # seed random numbers to make calculation deterministic\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # initialize weights randomly with mean 0\n",
    "        self.w = 2*np.random.random((input_dim,1)) - 1\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        # Number of iterations\n",
    "        self.n_iter = int(1e4)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        l1 = sigmoid(np.dot(X,self.w))\n",
    "        return l1\n",
    "    \n",
    "    def backward(self,X,y):\n",
    "        # do výsledku uložíme chybu\n",
    "        res =  y - self.forward(X)\n",
    "        # tu propagujeme dále\n",
    "        # - spočteme derivaci ztrátové funkce\n",
    "        res = -2*res\n",
    "        # - vynásobíme derivací aktivační funkce \n",
    "        res = res*sigmoid(np.dot(X, self.w), deriv = True)\n",
    "        # - vynásobíme derivací vnitřního potenciálu\n",
    "        res = np.dot(X.T, res)\n",
    "        # výsledkem je gradient - jakožto vektor derivací podle obou koeficientů\n",
    "        grad = res\n",
    "        # print(grad)\n",
    "        \n",
    "        # provedeme update vah\n",
    "        self.w -= self.lr*grad\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        for _ in range(self.n_iter):\n",
    "            # backward step\n",
    "            self.backward(X,y)\n",
    "   \n",
    "    def predict(self,X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pojďme to vyzkoušet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(input_dim=2)\n",
    "nn.fit(X,y)\n",
    "nn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnáme s originálním výstupem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematický je bod $[0,0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koeficienty\n",
    "nn.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualizace rozhodovací hranice\n",
    "$$w_1x_1 + w_2 x_2 = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.linspace(0,1,100)\n",
    "\n",
    "plot_X_y(X,y)\n",
    "plt.plot(xgrid, -nn.w[0]/nn.w[1]*xgrid)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problémem je počátek. Je to způsobeno tím, že rozhodovací hranice jím vždy prochází!**\n",
    "\n",
    "### Task 1 - opravte to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "\n",
    "class NeuralNet2(NeuralNet):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet2(input_dim=2)\n",
    "nn.fit(X,y)\n",
    "print(nn.predict(X))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.linspace(0,1,100)\n",
    "\n",
    "plot_X_y(X,y)\n",
    "plt.plot(xgrid, - nn.w[0]/nn.w[2] -nn.w[1]/nn.w[2]*xgrid)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - zkuste, zda tento jednoduchý perceptron dokáže zvládnout logické funkce AND a OR\n",
    "- pokaždé vykreslete rozhodovací hranici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problém nastane až s logickou funkcí XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X_xor = np.array([[0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1]])\n",
    "    \n",
    "# output dataset            \n",
    "y_xor = np.array([[0,1,1,0]]).T\n",
    "\n",
    "plot_X_y(X_xor,y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet2(input_dim=2)\n",
    "nn.fit(X_xor,y_xor)\n",
    "print(nn.predict(X_xor))\n",
    "print(y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.linspace(0,1,100)\n",
    "\n",
    "plot_X_y(X_xor,y_xor)\n",
    "plt.plot(xgrid, - nn.w[0]/nn.w[2] -nn.w[1]/nn.w[2]*xgrid)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun fact:** `sklearn`'s implementation with a hidden layer can train badly! This is because you get stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "for i in range(10):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(2,), random_state=i)\n",
    "    mlp.fit(X_xor,y_xor)\n",
    "    print(\"Error with seed {}: {}\".format(i,mlp.score(X_xor,y_xor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - implementujte vícevrstvou síť, která zvládne XOR\n",
    "- nejprve zkuste, zda to zvládne scikit s více vrstvami\n",
    "- poté zkuste vlastní řešení pomocí co nejmenšího počtu neuronů (jedna skrytá vrstva se dvěma neurony a RELU stačí) \n",
    "- neimplementujte backpropagation (který to moc nezvládá) ale váhy nastavte natvrdo\n",
    "\n",
    "$$ \\text{RELU}(x) = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonusový Task - Prozkoumejte [Tensorflow playground](http://playground.tensorflow.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
