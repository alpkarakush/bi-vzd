{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k Nearest Neighbours and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('house-prices-train.csv')\n",
    "data.SalePrice = np.log1p(data.SalePrice)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning (copy/pasted from the previous tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_categories(df, mappers, dummies=False):\n",
    "    le = LabelEncoder()\n",
    "    for col in df.select_dtypes('object').columns:\n",
    "        if col not in mappers and df[col].nunique() < 30:\n",
    "            df[col] = df[col].fillna('NaN')\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            if dummies:\n",
    "                prefix = 'd_' + col\n",
    "                df = pd.concat([df.drop(columns=[col]), pd.get_dummies(df[col], prefix=prefix)], axis=1)\n",
    "        elif col in mappers:\n",
    "            df[col] = df[col].replace(mappers[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('house-prices-train.csv')\n",
    "data.SalePrice = np.log1p(data.SalePrice)\n",
    "ordinal_cols_mappers = {\n",
    "    'KitchenQual': {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "}\n",
    "data = encode_categories(data, ordinal_cols_mappers, True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * The nature of kNN algorithms means that using kNN with nominal features is troublesome.\n",
    "  * To overcome this, one can adopt one of these strategies:\n",
    "    * Drop nominal features (and possibly keep the ordinal one if there is some meaning for measuring the distance).\n",
    "    * Replace nominal features with dummies using one-hot encoding.\n",
    "    * Use some [more sophisticated metrics](https://www.researchgate.net/publication/220907006_Similarity_Measures_for_Categorical_Data_A_Comparative_Evaluation) capable of measuring the similarity of nominal features.\n",
    "  * We will give a try to the first two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(data.drop(columns=['SalePrice']), data.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * There is a problem with missing values of numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,data.isnull().sum() > 0].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do:\n",
    "  * Drop the data points with missing values. But we do not have enough data for this.\n",
    "  * We can replace the missings with respective means. But it is too simple, and we have some dignity!\n",
    "  * We can predict the missing values from the rest of the data! That's it! We will use the kNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: predict the missing values using kNN\n",
    "\n",
    "The idea is this (assume we want to fill missing values in `LotFrontage` column):\n",
    "  * Split the dataset into two parts: \n",
    "    * `D1` = contaning the lines with missing values in `LotFrontage` column, \n",
    "    * `D2` = the rest of the data.\n",
    "  * Save the column `D2.LotFrontage` to `Y` and the remaining columns to `X` (exclude some columns if needed). The same columns of `D1` save to `X2`.\n",
    "  * Fit a model (we use the kNN) to predict `Y` using `X`.\n",
    "  * Use this model to predict the missing values of `LotFrontage` using the `X2` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans(df, cols_nan, params):\n",
    "    ### your code goes here\n",
    "    for col_nan in cols_nan:\n",
    "        Y = df[df[col_nan].notnull()][col_nan]\n",
    "        X = df[df[col_nan].notnull()].drop(columns=np.append(cols_nan.values,'Id'))\n",
    "        X2 = df[df[col_nan].isnull()].drop(columns=np.append(cols_nan.values,'Id'))\n",
    "        Y2idx = df[df[col_nan].isnull()].index \n",
    "        # this is optional, but some treatment of nominal features is needed\n",
    "        X = X.select_dtypes(['float64', 'int64']) \n",
    "        X2 = X2.select_dtypes(['float64', 'int64']) \n",
    "        kNN = KNeighborsRegressor(**params)\n",
    "        kNN.fit(X,Y)\n",
    "        Ypredict = kNN.predict(X2)\n",
    "        df.loc[Y2idx,col_nan] = Ypredict\n",
    "    ###\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that we have some meaningful results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "cols_nan = df.loc[:,data.isnull().sum() > 0].columns\n",
    "params = {\n",
    "        'n_neighbors': 5\n",
    "}\n",
    "df = replace_nans(df, cols_nan, params)\n",
    "display(data[cols_nan].describe())\n",
    "display(dataNoNan[cols_nan].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and hyperparameter tuning\n",
    "\n",
    "  * Assume we want to go through the following values of the kNN hyperparameters.\n",
    "  * Beside this, we also want to see the effect of different strategies of \n",
    "    * how to deal with nominal features (ignoring them, using dummies), \n",
    "    * how to normalise the data (no normalising vs normalising)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data = data.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: implement cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, folds, model, dummies = False):\n",
    "    averageRMSLE = 0\n",
    "    np.random.seed(seed=654) # this must be here, explain WHY!\n",
    "    ### Your code goes here\n",
    "    if not dummies:\n",
    "        X = X.loc[:, X.nunique() > 3]\n",
    "    idx = np.random.randint(folds, size=y.shape[0])\n",
    "    X['cv_fold'] = idx\n",
    "    for k in range(folds):\n",
    "        idx_train = X[X.cv_fold != k].index\n",
    "        idx_val = X[X.cv_fold == k].index\n",
    "        Xt = X.loc[idx_train,:]\n",
    "        Xv = X.loc[idx_val,:]\n",
    "        yt = y[idx_train]\n",
    "        yv = y[idx_val]\n",
    "        model.fit(Xt.drop(columns=['cv_fold']), yt)\n",
    "        ypred = model.predict(Xv.drop(columns=['cv_fold']))\n",
    "        RMSLE = math.sqrt(metrics.mean_squared_error(yv, ypred))\n",
    "        averageRMSLE = averageRMSLE + RMSLE/folds\n",
    "    X.drop(columns=['cv_fold'])\n",
    "    ###\n",
    "    return averageRMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: try kNN with and without normalisation/dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "param_grid = {\n",
    "    'n_neighbors' : range(1,20),\n",
    "    'p': range(1,5),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "dummies = True\n",
    "param_comb = ParameterGrid(param_grid)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data.drop(columns=['SalePrice']), \n",
    "                                                data.SalePrice, \n",
    "                                                test_size=0.25, \n",
    "                                                random_state=6548)\n",
    "### your code doing normalisation goes here:\n",
    "# to avoid devision by zero:\n",
    "one_val_cols = Xtrain.loc[:,Xtrain.max(axis=0) - Xtrain.min(axis=0) == 0].columns \n",
    "Xtrain.drop(columns=one_val_cols, inplace=True)\n",
    "Xtest.drop(columns=one_val_cols, inplace=True)\n",
    "Xtrain = (Xtrain - Xtrain.min(axis=0)) / (Xtrain.max(axis=0) - Xtrain.min(axis=0))\n",
    "Xtest = (Xtest - Xtest.min(axis=0)) / (Xtest.max(axis=0) - Xtest.min(axis=0))\n",
    "###\n",
    "crossval_err = []\n",
    "for params in param_comb:\n",
    "    kNN = KNeighborsRegressor(**params)\n",
    "    averageRMSLE = cross_val(Xtrain.copy(), ytrain, 12, kNN, dummies)\n",
    "    crossval_err.append(averageRMSLE)\n",
    "crossval_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_params = param_comb[np.argmin(crossval_err)]\n",
    "kNN = KNeighborsRegressor(**best_params)\n",
    "if not dummies:\n",
    "    Xtrain = Xtrain.loc[:, Xtrain.nunique() > 3]\n",
    "    Xtest = Xtest.loc[:, Xtrain.columns]\n",
    "Xtest.fillna(0, inplace=True)\n",
    "print(Xtrain.shape, Xtest.shape)\n",
    "\n",
    "kNN.fit(Xtrain, ytrain)\n",
    "ypred = kNN.predict(Xtest)\n",
    "best_RMSLE = math.sqrt(metrics.mean_squared_error(ytest, ypred))\n",
    "print('RMSLE (test): {0:.6f}'.format(best_RMSLE))\n",
    "print('best parameters:', best_params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RMSLE (test): 0.198943\n",
    "best parameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 10} and dummies = True\n",
    "\n",
    "RMSLE (test): 0.200576\n",
    "best parameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 10} and dummies = False\n",
    "\n",
    "RMSLE (test): 0.179897\n",
    "best parameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 9} and dummies = False\n",
    "\n",
    "RMSLE (test): 0.198415\n",
    "best parameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course packages in `sklearn` for Cross-Validation and normalisation:\n",
    "  * [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "  * [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "  * [cross_validate](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "  * [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "  * Normalised data are all localised in the $n$-dimensional cube with sides of length one.\n",
    "  * The diagonal of this cube equals $\\sqrt{n}$.\n",
    "  * The curse of dimensionality says that higher the dimension the nearest neighbours get further and further.\n",
    "  * To measure this effect, we will increase the dimension and observe the ration of the diagonal and the mean distance of the nearest neighbours.\n",
    "\n",
    "**Try to experiment with the `n_neighbors` parameter!** What is the influence of the number of neigbours and the mean distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain and Xtest should be normalized here\n",
    "mean_dist_ratio = []\n",
    "for k in range(1,30):\n",
    "    kNN = KNeighborsRegressor(n_neighbors=150, p=2)\n",
    "    kNN.fit(Xtrain.iloc[:,0:k], ytrain)\n",
    "    dist, nn = kNN.kneighbors(Xtest.iloc[:,0:k])\n",
    "    mean_dist_ratio.append(np.mean(dist)/math.sqrt(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel('dimensions')\n",
    "plt.plot(range(1,len(mean_dist_ratio)+1),mean_dist_ratio,'bo-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
